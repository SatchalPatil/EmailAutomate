import logging
import ollama  

logger = logging.getLogger(__name__)

def process_general_chat(user_input):
    """
    Processes a general conversation input with the LLM.
    """
    try:
        response = ollama.chat(model='qwen2.5:3B', messages=[{"role": "user", "content": user_input}])
        chat_response = response['message']['content']
        logger.info("General chat response generated by LLM.")
        return chat_response
    except Exception as e:
        logger.error(f"Error in general chat processing: {e}")
        return "Sorry, something went wrong with processing your message."
